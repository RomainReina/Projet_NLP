{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projet_NLP_REINAROMAIN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e3a68f9d24b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import Python libraries and helper functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Import Python libraries and helper functions \n",
    "from keras.datasets import imdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import LSTM, Activation, Dropout, Dense, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Model\n",
    "import string\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.text import Text\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation du dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "0        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "0          scotthamilton   \n",
       "1               mattycus   \n",
       "2                ElleCTF   \n",
       "3                 Karoli   \n",
       "4               joy_wolf   \n",
       "...                  ...   \n",
       "1599994  AmandaMarie1028   \n",
       "1599995      TheWDBoards   \n",
       "1599996           bpbabe   \n",
       "1599997     tinydiamondz   \n",
       "1599998   RyanTrevMorris   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0        is upset that he can't update his Facebook by ...                                                                   \n",
       "1        @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2          my whole body feels itchy and like its on fire                                                                    \n",
       "3        @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                            @Kwesidei not the whole crew                                                                    \n",
       "...                                                    ...                                                                   \n",
       "1599994  Just woke up. Having no school is the best fee...                                                                   \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...                                                                   \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...                                                                   \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...                                                                   \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...                                                                   \n",
       "\n",
       "[1599999 rows x 6 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = pd.read_csv('training.1600000.processed.noemoticon.csv')\n",
    "csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "n8gjHp8jwH6j"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index                                           comments\n",
       "0            0  is upset that he can't update his Facebook by ...\n",
       "1            0  @Kenichan I dived many times for the ball. Man...\n",
       "2            0    my whole body feels itchy and like its on fire \n",
       "3            0  @nationwideclass no, it's not behaving at all....\n",
       "4            0                      @Kwesidei not the whole crew \n",
       "...        ...                                                ...\n",
       "1599994      4  Just woke up. Having no school is the best fee...\n",
       "1599995      4  TheWDB.com - Very cool to hear old Walt interv...\n",
       "1599996      4  Are you ready for your MoJo Makeover? Ask me f...\n",
       "1599997      4  Happy 38th Birthday to my boo of alll time!!! ...\n",
       "1599998      4  happy #charitytuesday @theNSPCC @SparksCharity...\n",
       "\n",
       "[1599999 rows x 2 columns]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = csv.drop(['1467810369','Mon Apr 06 22:19:45 PDT 2009','NO_QUERY','_TheSpecialOne_'],axis = 1)\n",
    "data.columns = ['index', 'comments']\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0</td>\n",
       "      <td>wanttss to go out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0</td>\n",
       "      <td>Is not going to sleep tonite.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0</td>\n",
       "      <td>too worried and tired to post tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0</td>\n",
       "      <td>couldn't get shit done today ~ i'm so screwed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0</td>\n",
       "      <td>Job Interview in Cardiff today, wish me luck! ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                           comments\n",
       "0        0  is upset that he can't update his Facebook by ...\n",
       "1        0  @Kenichan I dived many times for the ball. Man...\n",
       "2        0    my whole body feels itchy and like its on fire \n",
       "3        0  @nationwideclass no, it's not behaving at all....\n",
       "4        0                      @Kwesidei not the whole crew \n",
       "..     ...                                                ...\n",
       "195      0                                 wanttss to go out \n",
       "196      0                     Is not going to sleep tonite. \n",
       "197      0             too worried and tired to post tonight \n",
       "198      0     couldn't get shit done today ~ i'm so screwed \n",
       "199      0  Job Interview in Cardiff today, wish me luck! ...\n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#j'en veuxx que 200\n",
    "data = data[:200]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)\n",
    "data_train = data['comments']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement de texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\ismar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3417: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "C:\\Users\\ismar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\ismar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['upset', 'update', 'facebook', 'texting', 'might', 'cry', 'result', 'school', 'today', 'also', 'blah']\n",
      "11\n",
      "['kenichan', 'dived', 'many', 'time', 'ball', 'managed', 'save', '50', 'rest', 'go', 'bound']\n",
      "11\n",
      "['whole', 'body', 'feel', 'itchy', 'like', 'fire']\n",
      "6\n",
      "['nationwideclass', 'behaving', 'mad', 'see']\n",
      "4\n",
      "['kwesidei', 'whole', 'crew']\n",
      "3\n",
      "['need', 'hug']\n",
      "2\n",
      "['loltrish', 'hey', 'long', 'time', 'see', 'yes', 'rain', 'bit', 'bit', 'lol', 'fine', 'thanks']\n",
      "12\n",
      "['tatiana_k', 'nope']\n",
      "2\n",
      "['twittera', 'que', 'muera']\n",
      "3\n",
      "['spring', 'break', 'plain', 'city', 'snowing']\n",
      "5\n",
      "['pierced', 'ear']\n",
      "2\n",
      "['caregiving', 'bear', 'watch', 'thought', 'ua', 'loss', 'embarrassing']\n",
      "7\n",
      "['octolinz16', 'count', 'idk', 'either', 'never', 'talk', 'anymore']\n",
      "7\n",
      "['smarrison', 'would', 'first', 'gun', 'really', 'though', 'zac', 'snyder', 'doucheclown']\n",
      "9\n",
      "['iamjazzyfizzle', 'wish', 'got', 'watch', 'miss', 'iamlilnicki', 'premiere']\n",
      "7\n",
      "['hollis', 'death', 'scene', 'hurt', 'severely', 'watch', 'film', 'wry', 'director', 'cut']\n",
      "10\n",
      "['file', 'tax']\n",
      "2\n",
      "['lettya', 'ahh', 'ive', 'always', 'wanted', 'see', 'rent', 'love', 'soundtrack']\n",
      "9\n",
      "['fakerpattypattz', 'oh', 'dear', 'drinking', 'forgotten', 'table', 'drink']\n",
      "7\n",
      "['alydesigns', 'day', 'get', 'much', 'done']\n",
      "5\n",
      "['one', 'friend', 'called', 'asked', 'meet', 'mid', 'valley', 'today', 'time', 'sigh']\n",
      "10\n",
      "['angry_barista', 'baked', 'cake', 'ated']\n",
      "4\n",
      "['week', 'going', 'hoped']\n",
      "3\n",
      "['blagh', 'class', '8', 'tomorrow']\n",
      "4\n",
      "['hate', 'call', 'wake', 'people']\n",
      "4\n",
      "['going', 'cry', 'sleep', 'watching', 'marley']\n",
      "5\n",
      "['im', 'sad', 'miss', 'lilly']\n",
      "4\n",
      "['ooooh', 'lol', 'leslie', 'ok', 'leslie', 'get', 'mad']\n",
      "7\n",
      "['meh', 'almost', 'lover', 'exception', 'track', 'get', 'depressed', 'every', 'time']\n",
      "9\n",
      "['some1', 'hacked', 'account', 'aim', 'make', 'new', 'one']\n",
      "7\n",
      "['alielayus', 'want', 'go', 'promote', 'gear', 'groove', 'unfornately', 'ride', 'may', 'b', 'going', 'one', 'anaheim', 'may', 'though']\n",
      "15\n",
      "['thought', 'sleeping', 'option', 'tomorrow', 'realizing', 'evaluation', 'morning', 'work', 'afternoon']\n",
      "9\n",
      "['julieebaby', 'awe', 'love', '1', 'miss']\n",
      "5\n",
      "['humpninja', 'cry', 'asian', 'eye', 'sleep', 'night']\n",
      "6\n",
      "['ok', 'sick', 'spent', 'hour', 'sitting', 'shower', 'cause', 'sick', 'stand', 'held', 'back', 'puke', 'like', 'champ', 'bed']\n",
      "15\n",
      "['cocomix04', 'ill', 'tell', 'ya', 'story', 'later', 'good', 'day', 'ill', 'workin', 'like', 'three', 'hour']\n",
      "13\n",
      "['missxu', 'sorry', 'bed', 'time', 'came', 'gmt', '1', 'http', 'gd', 'fnge']\n",
      "10\n",
      "['fleurylis', 'either', 'depressing', 'think', 'even', 'want', 'know', 'kid', 'suitcase']\n",
      "9\n",
      "['bed', 'class', '8', '12', 'work', '12', '3', 'gym', '3', '5', '6', 'class', '6', '10', 'another', 'day', 'gonna', 'fly', 'miss', 'girlfriend']\n",
      "20\n",
      "['really', 'feel', 'like', 'getting', 'today', 'got', 'study', 'tomorrow', 'practical', 'exam']\n",
      "10\n",
      "['reason', 'teardrop', 'guitar', 'one', 'enough', 'break', 'heart']\n",
      "7\n",
      "['sad', 'sad', 'sad', 'know', 'hate', 'feeling', 'wanna', 'sleep', 'still']\n",
      "9\n",
      "['jonathanrknight', 'awww', 'soo', 'wish', 'see', 'finally', 'comfortable', 'im', 'sad', 'missed']\n",
      "10\n",
      "['falling', 'asleep', 'heard', 'tracy', 'girl', 'body', 'found', 'sad', 'heart', 'break', 'family']\n",
      "11\n",
      "['viennah', 'yay', 'happy', 'job', 'also', 'mean', 'le', 'time']\n",
      "8\n",
      "['checked', 'user', 'timeline', 'blackberry', 'look', 'like', 'twanking', 'still', 'happening', 'ppl', 'still', 'probs', 'w', 'bgs', 'uids']\n",
      "15\n",
      "['oh', 'man', 'ironing', 'jeancjumbe', 'fave', 'top', 'wear', 'meeting', 'burnt']\n",
      "9\n",
      "['strangely', 'sad', 'lilo', 'samro', 'breaking']\n",
      "5\n",
      "['tea', 'oh', 'sorry', 'think', 'retweeting']\n",
      "5\n",
      "['broadband', 'plan', 'massive', 'broken', 'promise', 'http', 'tinyurl', 'com', 'dcuc33', 'via', 'www', 'diigo', 'com', 'tautao', 'still', 'waiting', 'broadband']\n",
      "17\n",
      "['localtweeps', 'wow', 'ton', 'reply', 'may', 'unfollow', 'see', 'friend', 'tweet', 'scrolling', 'feed', 'lot']\n",
      "12\n",
      "['duck', 'chicken', 'taking', 'wayyy', 'long', 'hatch']\n",
      "6\n",
      "['put', 'vacation', 'photo', 'online', 'yr', 'ago', 'pc', 'crashed', 'forget', 'name', 'site']\n",
      "11\n",
      "['need', 'hug']\n",
      "2\n",
      "['andywana', 'sure', 'po', 'much', 'want', 'dont', 'think', 'trade', 'away', 'company', 'asset', 'sorry', 'andy']\n",
      "13\n",
      "['oanhlove', 'hate', 'happens']\n",
      "3\n",
      "['sad', 'feeling', 'dallas', 'going', 'show', 'gotta', 'say', 'though', 'think', 'show', 'would', 'use', 'music', 'game', 'mmm']\n",
      "15\n",
      "['ugh', '92', 'degree', 'tomorrow']\n",
      "4\n",
      "['u', 'move', 'thought', 'u', 'already', 'sd', 'hmmm', 'random', 'u', 'found', 'glad', 'hear', 'yer', 'well']\n",
      "14\n",
      "['batmanyng', 'miss', 'ps3', 'commission', 'wutcha', 'playing', 'copped', 'blood', 'sand']\n",
      "9\n",
      "['leaving', 'parking', 'lot', 'work']\n",
      "4\n",
      "['life', 'cool']\n",
      "2\n",
      "['sadly', 'though', 'never', 'gotten', 'experience', 'post', 'coitus', 'cigarette', 'never']\n",
      "9\n",
      "['nice', 'day', 'bad', 'rain', 'come', 'tomorrow', '5am']\n",
      "7\n",
      "['starrbby', 'bad', 'around', 'lost', 'job', 'even', 'pay', 'phone', 'bill', 'lmao', 'aw', 'shuck']\n",
      "12\n",
      "['damm', 'back', 'school', 'tomorrow']\n",
      "4\n",
      "['mo', 'job', 'money', 'hell', 'min', 'wage', '4', 'f', 'n', 'clam', 'hour']\n",
      "11\n",
      "['katortiz', 'forever', 'see', 'soon']\n",
      "4\n",
      "['lt_algonquin', 'agreed', 'saw', 'failwhale', 'allllll', 'day', 'today']\n",
      "7\n",
      "['jdarter', 'oh', 'haha', 'dude', 'dont', 'really', 'look', 'em', 'unless', 'someone', 'say', 'hey', 'added', 'sorry', 'terrible', 'need', 'pop']\n",
      "17\n",
      "['ninjen', 'sure', 'right', 'need', 'start', 'working', 'nikster', 'jared', 'least']\n",
      "9\n",
      "['really', 'hate', 'people', 'dis', 'band', 'trace', 'clearly', 'ugly']\n",
      "8\n",
      "['gym', 'attire', 'today', 'puma', 'singlet', 'adidas', 'short', 'black', 'business', 'sock', 'leather', 'shoe', 'lucky', 'run', 'cute', 'girl']\n",
      "16\n",
      "['show', 'location', 'http', 'twitpic', 'com', '2y2es']\n",
      "6\n",
      "['picnic', 'phone', 'smell', 'like', 'citrus']\n",
      "5\n",
      "['ashleyac', 'donkey', 'sensitive', 'comment', 'nevertheless', 'glad', 'see', 'mug', 'asap', 'charger', 'still', 'awol']\n",
      "12\n",
      "['new', 'csi', 'tonight', 'fml']\n",
      "4\n",
      "['think', 'arm', 'sore', 'tennis']\n",
      "4\n",
      "['wonder', 'someone', 'u', 'like', 'much', 'make', 'unhappy', 'split', 'seccond', 'depressed']\n",
      "10\n",
      "['sleep', 'soon', 'hate', 'saying', 'bye', 'see', 'tomorrow', 'night']\n",
      "8\n",
      "['statravelau', 'got', 'ur', 'newsletter', 'fare', 'really', 'unbelievable', 'shame', 'already', 'booked', 'paid', 'mine']\n",
      "12\n",
      "['missin', 'boo']\n",
      "2\n",
      "['markhardy1974', 'itm']\n",
      "2\n",
      "['damn', 'chalk', 'chalkboard', 'useless']\n",
      "4\n",
      "['blast', 'getty', 'villa', 'hate', 'sore', 'throat', 'day', 'getting', 'worse']\n",
      "9\n",
      "['msdrama', 'hey', 'missed', 'ya', 'meeting', 'sup', 'mama']\n",
      "7\n",
      "['tummy', 'hurt', 'wonder', 'hypnosis', 'anything', 'working', 'get', 'stop', 'smoking']\n",
      "9\n",
      "['always', 'fat', 'one']\n",
      "3\n",
      "['januarycrimson', 'sorry', 'babe', 'fam', 'annoys', 'thankfully', 'asleep', 'right', 'muahaha', 'evil', 'laugh']\n",
      "11\n",
      "['hollywoodheat', 'paid', 'attention', 'covered', 'photoshop', 'webpage', 'design', 'class', 'undergrad']\n",
      "9\n",
      "['wednesday', 'b', 'day', 'know', '2']\n",
      "5\n",
      "['poor', 'cameron', 'hill']\n",
      "3\n",
      "['pray', 'please', 'ex', 'threatening', 'start', 'sh', 'baby', '1st', 'birthday', 'party', 'jerk', 'still', 'headache']\n",
      "13\n",
      "['makeherfamous', 'hmm', 'u', 'really', 'enjoy', 'problem', 'constant', 'u', 'think', 'thing', 'find', 'someone', 'ulike']\n",
      "13\n",
      "['strider', 'sick', 'little', 'puppy', 'http', 'apps', 'facebook', 'com', 'dogbook', 'profile', 'view', '5248435']\n",
      "12\n",
      "['rylee', 'grace', 'wana', 'go', 'steve', 'party', 'sadly', 'since', 'easter', 'wnt', 'b', 'able', '2', 'much', 'ohh', 'well']\n",
      "16\n",
      "['hey', 'actually', 'one', 'bracket', 'pool', 'bad', 'one', 'money']\n",
      "8\n",
      "['stark', 'follow', 'either', 'work']\n",
      "4\n",
      "['bad', 'nite', 'favorite', 'team', 'astros', 'spartan', 'lose', 'nite', 'w', 'good']\n",
      "10\n",
      "['body', 'missing', 'northern', 'calif', 'girl', 'found', 'police', 'found', 'remains', 'missing', 'northern', 'california', 'girl', 'http', 'tr', 'im', 'imji']\n",
      "17\n",
      "['mangaaa', 'hope', 'increase', 'capacity', 'fast', 'yesterday', 'pain', 'got', 'fail', 'whale', '15', 'time', '2', 'hour']\n",
      "14\n",
      "['behind', 'class', 'work']\n",
      "3\n",
      "['watching', 'quot', 'house', 'quot']\n",
      "4\n",
      "['kpreyes', 'remember', 'bum', 'leg', 'strike', 'back', 'time', 'serious']\n",
      "8\n",
      "['paradisej', 'cool', 'kind', 'complaint', 'laptop', 'online', 'overheating', 'recall']\n",
      "8\n",
      "['emily', 'glad', 'mommy', 'done', 'training', 'new', 'job', 'miss', 'http', 'apps', 'facebook', 'com', 'dogbook', 'profile', 'view', '6176014']\n",
      "16\n",
      "['would', 'rather', 'first', 'party', 'send', 'bad', 'message', '3rd', 'party', 'send', 'mixed', 'one', 'sophmore', 'year']\n",
      "14\n",
      "['henkuyinepu', 'overrated']\n",
      "2\n",
      "['marykatherine_q', 'know', 'heard', 'afternoon', 'wondered', 'thing', 'moscow', 'behind', 'time']\n",
      "9\n",
      "['laying', 'bed', 'voice']\n",
      "3\n",
      "['sooo', 'sad', 'killed', 'kutner', 'house', 'whyyyyyyyy']\n",
      "6\n",
      "['jacobsummers', 'sorry', 'tell', 'mea', 'culpa', 'really', 'sorry']\n",
      "7\n",
      "['alliana07', 'make', 'sense', 'suicide', 'thing', 'refuse', 'believe', 'actually', 'happened']\n",
      "9\n",
      "['salancaster', 'hope', 'ok']\n",
      "3\n",
      "['mercedesashley', 'damn', 'grind', 'inspirational', 'saddening', 'time', 'want', 'stop', 'cuz', 'like', 'u', 'much', 'love']\n",
      "13\n",
      "['hibanick', 'yeah', 'aw', 'know', 'wudnt', 'stand', 'chance']\n",
      "7\n",
      "['ugh', 'cant', 'sleep', '1', '30am']\n",
      "5\n",
      "['hanging', 'crooner', 'wanna', 'sing', 'suck']\n",
      "5\n",
      "['erre_sc', 'aaw', 'miss', 'ya', 'im', 'leaving', 'bh', 'tomorrow', 'quot', 'morning', 'quot', 'think', 'aww', 'wanna', 'go', 'beach', 'w', 'u', 'girl']\n",
      "19\n",
      "['pissed', 'asba', 'radio', 'station']\n",
      "4\n",
      "['wednesday', 'b', 'day', 'n', 'know', '2']\n",
      "6\n",
      "['know', 'life', 'flipped', 'upside', 'thought', 'head', 'ramen', 'sound', 'good']\n",
      "9\n",
      "['pain', 'back', 'side', 'hurt', 'mention', 'cry', 'made', 'fail']\n",
      "8\n",
      "['late', 'night', 'snack', 'glass', 'oj', 'b', 'c', 'quot', 'sickness', 'quot', 'back', 'sleep', 'ugh', 'hate', 'getting', 'sick']\n",
      "16\n",
      "['allyheman', 'big', 'fan', 'camilla', 'belle']\n",
      "5\n",
      "['grum', 'wah', 'see', 'clip', 'must', 'el', 'stupido', 'work', 'filter', 'wait', 'till', 'get', 'puter', 'something', 'else', '2', 'blame', 'ex', '4', 'broke', 'mine']\n",
      "21\n",
      "['week', 'seems', 'get', 'longer', 'longer', 'term', 'much', 'need', 'much', 'actually', 'going', 'get', 'done']\n",
      "13\n",
      "['cold']\n",
      "1\n",
      "['thecoolestout', 'ehhh', 'weather', 'gonna', 'take', 'turn', 'ugly', 'tomorrow']\n",
      "8\n",
      "['chelserlynn', 'haha', 'cooooold', 'still', 'go', 'show', 'incredible', 'stuff']\n",
      "8\n",
      "['hoping', 'tummy', 'rumble', 'go', 'away', 'soon']\n",
      "6\n",
      "['knights_', 'notice', 'told', 'working', 'tomorrow', 'called', 'agency', 'follow', 'said']\n",
      "9\n",
      "['almost', 'bedtime']\n",
      "2\n",
      "['missing', 'babe', 'long', 'alive', 'happy', 'yawwwnn', 'tired', 'love', 'imma', 'try', 'sleep', 'hopefully', 'headstart']\n",
      "13\n",
      "['agh', 'snow']\n",
      "2\n",
      "['miss', 'kenny', 'power']\n",
      "3\n",
      "['bridgetsbeaches', 'thank', 'letting', 'people', 'know', 'sad', 'direct', 'message', 'got', 'actually', 'bridget']\n",
      "11\n",
      "['india', 'missed', '100th', 'test', 'victory', 'n', '10th', 'consecutive', 'win', 'without', 'loss']\n",
      "11\n",
      "['jonathanrknight', 'guess']\n",
      "2\n",
      "['sadly', 'going', 'bed']\n",
      "3\n",
      "['ozesteph1992', 'shame', 'hear', 'stephan']\n",
      "4\n",
      "['mrsaintnick', 'hey', 'leavin', 'morning']\n",
      "4\n",
      "['intending', 'finish', 'editing', '536', 'page', 'novel', 'manuscript', 'tonight', 'probably', 'happen', '12', 'page', 'left']\n",
      "13\n",
      "['laid', 'around', 'much', 'today', 'head', 'hurt']\n",
      "6\n",
      "['twista202', 'still', 'read', '9th', 'amp', '10th', 'princess', 'diary', 'saving', 'francesca', 'made', 'cry', 'end', 'hmm', 'easy', 'book']\n",
      "16\n",
      "['nokia', '1110', 'died']\n",
      "3\n",
      "['mom', 'might', 'breast', 'cancer', 'find', 'anything', 'like', 'week', 'worried']\n",
      "9\n",
      "['going', 'sleep', 'hoping', 'tomorrow', 'better', 'day']\n",
      "6\n",
      "['rumblepurr', 'lol', 'wish', 'understood', 'daylight', 'saving', 'ended', 'though', 'breakfast', 'hour', 'later', 'keep', 'waking', 'kid']\n",
      "14\n",
      "['onemoreproject', 'lame']\n",
      "2\n",
      "['understand', 'really']\n",
      "2\n",
      "['hero', 'season']\n",
      "2\n",
      "['living', 'downtown', 'sure', 'much', 'fun']\n",
      "5\n",
      "['jonathanchard', 'calorie', 'wise', 'wish', 'junk', 'food', 'calorie', 'free', 'ate', 'thing', 'sour', 'skittle', 'big', 'as', 'cherry', 'coke']\n",
      "16\n",
      "['man', 'work', 'hard']\n",
      "3\n",
      "['getting', 'sick', 'time', 'hot', 'tea', 'studying', 'sleeeep']\n",
      "7\n",
      "['getting', 'eyebrow', 'waxed', 'pain']\n",
      "4\n",
      "['phantasy', 'star', 'yesterday', 'going', 'work']\n",
      "5\n",
      "['oh', 'got', 'macheist', '3', '0', 'apps', 'sweet', 'get', 'espresso', 'serial', 'though', 'although', 'said', 'sent', 'oh', 'well']\n",
      "16\n",
      "['picked', 'mich', 'st', 'win', 'get', 'go', 'feeling', 'pretty', 'good', 'pick', 'way', 'tonight', 'lost']\n",
      "13\n",
      "['alone', 'downstairs', 'working']\n",
      "3\n",
      "['feel', 'bad']\n",
      "2\n",
      "['ryanseacrest', 'hate', 'anoop', 'mean', 'seriously', 'kinda', 'mean']\n",
      "7\n",
      "['pinkserendipity', 'yes', 'sprint', '4g', 'baltimore', 'chicago', 'far']\n",
      "7\n",
      "['stuck', 'awake', 'middle', 'night', 'second', 'day', 'row', 'felt', 'terrible', 'yesterday']\n",
      "10\n",
      "['thanks', 'bursting', 'bubble']\n",
      "3\n",
      "['going', 'school', 'soon', 'find', 'anything', 'wear', 'gosh', 'hard']\n",
      "8\n",
      "['marieclr', 'serious', 'lol']\n",
      "3\n",
      "['naughtyhaughty', 'page', 'sooooo', 'long', 'got', 'deleted', 'sad', 'day', 'history']\n",
      "9\n",
      "['crazy', 'wind', 'today', 'birding', 'http', 'ff', 'im', '1xtti']\n",
      "8\n",
      "['currently', 'work']\n",
      "2\n",
      "['grrr', 'ipod', 'acting', 'weird', 'jai', 'ho', 'thinking', 'playing', 'full', 'song', 'ughh']\n",
      "11\n",
      "['penndbad', 'send', 'dvd', 'co', 'missed', 'heap', 'happy']\n",
      "7\n",
      "['see', 'big', 'deal', 'website']\n",
      "4\n",
      "['machineplay', 'sorry', 'go', 'therapyfail']\n",
      "4\n",
      "['colindemar', 'far', 'way', 'rail', 'tip']\n",
      "5\n",
      "['still', 'swear', 'keep', 'losing', 'gaining', 'losing', 'gaining', 'tweeps', 'heart', 'wrenching']\n",
      "10\n",
      "['today', 'realized', 'good', 'hiding', 'thing', 'even', 'find']\n",
      "7\n",
      "['staying', 'friend', 'house', 'house', 'sitting', 'neighbor', 'loud', 'party']\n",
      "8\n",
      "['dannyvegasbaby', 'danny', 'im', 'upset', 'wasnt', 'watch', 'live', 'chat', 'car', '3', 'hour', 'trip', 'im', 'soooo', 'upset']\n",
      "15\n",
      "['check', 'mug', 'http', 'www', 'erika', 'obscura', 'blogspot', 'com']\n",
      "8\n",
      "['border', 'closed', '10']\n",
      "3\n",
      "['downloading', 'nin', 'new', 'album', 'quot', 'slip', 'quot', 'hell', 'come', 'behind', 'time', 'day']\n",
      "12\n",
      "['woke', 'already', 'written', 'e', 'mail', 'go', 'early', 'university', 'today', 'teach', '8', '30']\n",
      "12\n",
      "['watching', 'hill', 'making', 'sad']\n",
      "4\n",
      "['many', 'channel', 'yet', 'boring', 'lazy', 'day', 'may', 'find', 'hobby']\n",
      "9\n",
      "['supersport', 'miss', 'buddy', 'ill', 'ny', '25th']\n",
      "6\n",
      "['robluketic', 'love', 'french', 'tell', 'people', 'south', 'qtr', 'french', 'snarl', 'french', 'beautiful', 'people']\n",
      "12\n",
      "['opps', 'said', 'still', 'got', 'one', 'day', 'remain', 'problem', 'come']\n",
      "9\n",
      "['activated', 'selfcontrol', 'block', 'early', 'meaning', 'check', 'new', 'qc', 'regularizing', 'internal', 'clock', 'might', 'difficult', 'fb']\n",
      "14\n",
      "['hillydop', 'oh']\n",
      "2\n",
      "['spencer', 'good', 'guy']\n",
      "3\n",
      "['goodlaura', 'reese', 'dying', 'ttsc', 'season', 'finale', 'next', 'week', '24', 'boring', 'madame', 'president', 'crazy', 'woman']\n",
      "14\n",
      "['jonathanrknight', 'hate', 'limited', 'letter', 'hope', 'guy', 'fine', 'pray', 'dog', 'well']\n",
      "10\n",
      "['get', 'shit', 'done', 'today', 'screwed']\n",
      "5\n",
      "['wanttss', 'go']\n",
      "2\n",
      "['going', 'sleep', 'tonite']\n",
      "3\n",
      "['worried', 'tired', 'post', 'tonight']\n",
      "4\n",
      "['get', 'shit', 'done', 'today', 'screwed']\n",
      "5\n",
      "['job', 'interview', 'cardiff', 'today', 'wish', 'luck', 'got', '3', 'hour', 'sleep']\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ismar\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "#Minuscule\n",
    "for k in range(len(data_train)):\n",
    "    data_train[k] = data_train[k].lower()\n",
    "    \n",
    "#Tokenization\n",
    "tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "\n",
    "for k in range(len(data_train)):\n",
    "    data_train[k] = tokenizer.tokenize(data_train[k])\n",
    "    \n",
    "#Stopwords\n",
    "for k in range(len(data_train)):\n",
    "    data_train[k] = [w for w in data_train[k] if not w in list(nltk.corpus.stopwords.words('english'))]\n",
    "    \n",
    "#Lemmatization\n",
    "Word_Lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for k in range(len(data_train)):\n",
    "    data_train[k] = [Word_Lemmatizer.lemmatize(w) for w in data_train[k]]\n",
    "    print(data_train[k])\n",
    "    print(len(data_train[k]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-139-b4c865241e9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'<PAD>'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mQ1_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ1_train_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mQ2_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQ2_train_words\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_' is not defined"
     ]
    }
   ],
   "source": [
    "# Building the vocabulary with the train set         (this might take a minute)\n",
    "from collections import defaultdict\n",
    "\n",
    "vocab = defaultdict(lambda: 0)\n",
    "vocab['<PAD>'] = 1\n",
    "\n",
    "for idx in range(len(data_)):\n",
    "    Q1_train[idx] = nltk.word_tokenize(Q1_train_words[idx])\n",
    "    Q2_train[idx] = nltk.word_tokenize(Q2_train_words[idx])\n",
    "    q = Q1_train[idx] + Q2_train[idx]\n",
    "    for word in q:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = len(vocab) + 1\n",
    "print('The length of the vocabulary is: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNqGHYeohj+29AkujiX1HfR",
   "include_colab_link": true,
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}